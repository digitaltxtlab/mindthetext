xq()
q()
installed.packages(tm)
installed.packages('tm')
library(tm)
install.packages("tm")
library(tm)
ls(package:tm)
ls(package::tm)
ls("package:tm")
q()
installed.packages("text2vec")
install.packages("text2vec")
library(text2vec)
library("text2vec")
install.packages('readr')
library(readr)
library("readr")
library("readr")
library(tm)
install.packages(readr)
install.packages("readr")
library("readr")
?install.packages
install.packages("readr",dependencies = TRUE)
library("readr")
install.packages("readr",repos = getOption("repos"),dependencies = TRUE)
install.packages("readr",repos = getOption("repos"),dependencies = TRUE)
install.packages("readr")
library("readr")
install.packages("text2vec")
library("text2vec")
shell.exec("https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html")
?shell.exec
shell.exec("http://www.talkstats.com")
require(rjava)
library(rjava)
library(rJava)
install.packages("rJava", dependencies = TRUE)
q()
matt.v <- paste(scan('/home/kln/corpora/kjv_books/Matthew.txt',
what = 'character', sep='\n', encoding = 'UTF-8'), collapse = " ")
library(syuzhet)
library(tm)
# tokenize at sentence level
text_sent <- get_sentences(matt.v)
head(text_sent)
# AFINN sentiment lexicon
text_afinn <- get_sentiment(text_sent, method = 'afinn')
text_sent[which(text_afinn == max(text_afinn))]
text_sent[which(text_afinn == min(text_afinn))]
install.packages("dplyr")
library(dplyr)
library(stringr)
process_sentiment <- function (atext, amethod) {
chunkedtext <- data_frame(x = atext) %>%
group_by(linenumber = ceiling(row_number() / 10)) %>%
summarize(text = str_c(x, collapse = " "))
mySentiment <- data.frame(cbind(linenumber = chunkedtext$linenumber,
sentiment = get_sentiment(chunkedtext$text, method = amethod)))
}
matt_sentiment.df <- rbind(process_sentiment(text_sent,"afinn") %>% mutate(method = "AFINN"),
process_sentiment(text_sent,"bing") %>% mutate(method = "Bing et al"),
process_sentiment(text_sent,"nrc") %>% mutate(method = "NRC"))
library(ggplot2)
dev.new()
ggplot(data = matt_sentiment.df, aes(x = linenumber, y = sentiment, fill = method)) +
geom_bar(stat = "identity") +
facet_wrap(~method, nrow = 3) +
theme_minimal() +
ylab("Sentiment") +
labs(title = expression(paste("Sentiment in ", italic("Matthew"))))
wd <- '~/~/~/education/tm_R/some_r';
setwd(wd)
q()
install.packages('RODBC', dependencies = TRUE)
install.packages('RODBC', dependencies = TRUE)
library(RODBC)
myconn <-odbcConnect("mydsn", uid="Rob", pwd="aardvark")
crimedat <- sqlFetch(myconn, "Crime")
pundat <- sqlQuery(myconn, "select * from Punishment")
close(myconn)
myconn <-odbcConnect("mydsn", uid="Rob", pwd="aardvark")
library(RODBC)
myconn <-odbcConnect("mydsn", uid="Rob", pwd="aardvark")
remove.packages("RODBC")
library(RODBC)
remove.packages("RODBC")
install.packages('RODBC', dependencies = TRUE)
install.packages("RODBC", dependencies = TRUE)
library(RODBC)
myconn <-odbcConnect("mydsn", uid="Rob", pwd="aardvark")
sometext = 'i am a henious lalala. and who are you'
strsplit(sometext, "[ #]")
strsplit(sometext)
strsplit(sometext,' ')
strsplit(sometext, "[ #]")
somestr = 'this is a sent. and this is another sent. and yet another sent'
strsplit(somestr, "[ #]")
somestr = '# this is a sent. # and this is another sent.# and yet another sent'
print(strsplit(somestr, "[ #]"))
text.lower.v
text.lower.v <- tolower(somestr)
text.lower.v
text.words.v <- strsplit(text.lower.v, "[ #]")
text.words.v
somestr = ' # this is a sent. # and this is another sent. # and yet another sent'
print(strsplit(somestr, "[ #]"))
text.lower.v <- tolower(somestr) # casefolding
text.words.v <- strsplit(text.lower.v, "[ #]") # tokenize
text.words.v <- unlist(text.words.v) # transform list to vector
text.words.v <- text.words.v[which(text.words.v!="")] # remove blanks
text.words.v
?strsplit
kable(head(mtcars,3))
mtcars
head(mtcars,3)
head(mtcars,3)
library(ggplot2)
p=ggplot(mtcars,aes(y=Mileage,x=Weight))
p+geom_point(size=4)
head(mtcars,3)
library(ggplot2)
p=ggplot(mtcars,aes(y=Mileage,x=Weight))
p+geom_point(size=4)
p=ggplot(mtcars,aes(y=Mileage,x=Weight))
p
p=ggplot(mtcars,aes(y="Mileage",x="Weight"))
p+geom_point(size=4)
rm(list = ls())
wd <- '/home/kln/projects/geertz/mystic_anthro/code'
dd <- '/home/kln/projects/geertz/mystic_anthro/data'
setwd(wd)
library('tm')
library('topicmodels')
library('R.matlab')
preprocess <- function(v.cor){
tmp <- tm_map(v.cor, PlainTextDocument)
tmp <- tm_map(tmp, removePunctuation)
tmp <- tm_map(tmp, removeNumbers)
tmp <- tm_map(tmp, content_transformer(tolower))
tmp <- tm_map(tmp, removeWords, stopwords("english"))
tmp <- tm_map(tmp, stemDocument)
tmp <- tm_map(tmp, stripWhitespace)
cl.cor <- tm_map(tmp, PlainTextDocument)
return(cl.cor)
}
docsparse <- function(mindocs,dtm){
n = length(row.names(dtm))
sparse <- 1 - mindocs/n;
dtmreduce <- tm::removeSparseTerms(dtm, sparse)
return(dtmreduce)
}
datapath <- paste(dd,'/merge_data', sep = '')
vcor <- Corpus(DirSource(datapath, encoding  = "UTF-8"), readerControl = list(language="PlainTextDocument"))
docnames <- gsub("\\..*","",names(vcor))
vcor_cl <- preprocess(vcor)
mindoc = floor(.01*length(docnames))
dtm <- as.matrix(docsparse(mindoc,DocumentTermMatrix(vcor_cl)))
mindoc
rm(list = ls())
wd <- '/home/kln/projects/geertz/mystic_anthro/code'
dd <- '/home/kln/projects/geertz/mystic_anthro/data'
setwd(wd)
library('tm')
library('topicmodels')
library('R.matlab')
preprocess <- function(v.cor){
tmp <- tm_map(v.cor, PlainTextDocument)
tmp <- tm_map(tmp, removePunctuation)
tmp <- tm_map(tmp, removeNumbers)
tmp <- tm_map(tmp, content_transformer(tolower))
tmp <- tm_map(tmp, removeWords, stopwords("english"))
tmp <- tm_map(tmp, stemDocument)
tmp <- tm_map(tmp, stripWhitespace)
cl.cor <- tm_map(tmp, PlainTextDocument)
return(cl.cor)
}
docsparse <- function(mindocs,dtm){
n = length(row.names(dtm))
sparse <- 1 - mindocs/n;
dtmreduce <- tm::removeSparseTerms(dtm, sparse)
return(dtmreduce)
}
datapath <- paste(dd,'/merge_data', sep = '')
vcor <- Corpus(DirSource(datapath, encoding  = "UTF-8"), readerControl = list(language="PlainTextDocument"))
docnames <- gsub("\\..*","",names(vcor))
vcor_cl <- preprocess(vcor)
mindoc = floor(.005*length(docnames))
mindoc = floor(.002*length(docnames))
mindoc = floor(.003*length(docnames))
